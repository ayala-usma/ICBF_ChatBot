{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f870914-d26d-4bf7-8846-bca76b4b3c34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to deploy the model to be used by the QA Bot accelerator.  This notebook is available at https://github.com/databricks-industry-solutions/diy-llm-qa-bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b481371f-6a6f-4a48-9c08-38ad2447aeb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Introduction\n",
    "\n",
    "In this notebook, we will deploy the custom model registered with MLflow in the prior notebook and deploy it to Databricks model serving ([AWS](https://docs.databricks.com/machine-learning/model-serving/index.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/)).  Databricks model serving provides containerized deployment options for registered models thought which authenticated applications can interact with the model via a REST API.  This provides MLOps teams an easy way to deploy, manage and integrate their models with various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a66184e-db00-475f-abb1-f1447e101779",
     "showTitle": true,
     "title": "Get Config Settings"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./utils/config_utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a452a7b6-cad7-4dcb-be39-df29da425324",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from mlflow.utils.databricks_utils import get_databricks_host_creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0041ff-0dd1-4784-a252-0b3c3eb7ecdf",
     "showTitle": true,
     "title": "Retrieve the latest Production model version for deployment"
    }
   },
   "outputs": [],
   "source": [
    "latest_version = mlflow.MlflowClient().get_latest_versions(config['registered_model_name'], stages=['Production'])[0].version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684e1ab5-ba4c-4a52-a507-aaec865acf71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Step 1: Deploy Model Serving Endpoint\n",
    "\n",
    "Models may typically be deployed to model serving endpoints using either the Databricks workspace user-interface or a REST API.  Because our model depends on the deployment of a sensitive environment variable, we will need to leverage a relatively new model serving feature that is currently only available via the REST API.\n",
    "\n",
    "See our served model config below and notice the `env_vars` part of the served model config - you can now store a key in a secret scope and pass it to the model serving endpoint as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c026998e-c77a-4a74-8480-55399d74e551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "served_models = [\n",
    "    {\n",
    "      \"name\": \"current\",\n",
    "      \"model_name\": config['registered_model_name'],\n",
    "      \"model_version\": latest_version,\n",
    "      \"workload_size\": \"Small\",\n",
    "      \"scale_to_zero_enabled\": \"true\",\n",
    "      \"env_vars\": [{\n",
    "        \"env_var_name\": \"OPENAI_API_KEY\",\n",
    "        \"secret_scope\": config['openai_key_secret_scope'],\n",
    "        \"secret_key\": config['openai_key_secret_key'],\n",
    "      }]\n",
    "    }\n",
    "]\n",
    "traffic_config = {\"routes\": [{\"served_model_name\": \"current\", \"traffic_percentage\": \"100\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8671723b-68ad-47d6-8993-16d6c85a7385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: {'routes': [{'served_model_name': 'current', 'traffic_percentage': '100'}]}"
     ]
    }
   ],
   "source": [
    "traffic_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4ce598-f980-43fd-a5ff-92b7281b1027",
     "showTitle": true,
     "title": "Define functions to create or update endpoint according to our specification"
    }
   },
   "outputs": [],
   "source": [
    "def endpoint_exists():\n",
    "  \"\"\"Check if an endpoint with the serving_endpoint_name exists\"\"\"\n",
    "  url = f\"https://{serving_host}/api/2.0/serving-endpoints/{config['serving_endpoint_name']}\"\n",
    "  headers = { 'Authorization': f'Bearer {creds.token}' }\n",
    "  response = requests.get(url, headers=headers)\n",
    "  return response.status_code == 200\n",
    "\n",
    "def wait_for_endpoint():\n",
    "  \"\"\"Wait until deployment is ready, then return endpoint config\"\"\"\n",
    "  headers = { 'Authorization': f'Bearer {creds.token}' }\n",
    "  endpoint_url = f\"https://{serving_host}/api/2.0/serving-endpoints/{config['serving_endpoint_name']}\"\n",
    "  response = requests.request(method='GET', headers=headers, url=endpoint_url)\n",
    "  while response.json()[\"state\"][\"ready\"] == \"NOT_READY\" or response.json()[\"state\"][\"config_update\"] == \"IN_PROGRESS\" : # if the endpoint isn't ready, or undergoing config update\n",
    "    print(\"Waiting 30s for deployment or update to finish\")\n",
    "    time.sleep(30)\n",
    "    response = requests.request(method='GET', headers=headers, url=endpoint_url)\n",
    "    response.raise_for_status()\n",
    "  return response.json()\n",
    "\n",
    "def create_endpoint():\n",
    "  \"\"\"Create serving endpoint and wait for it to be ready\"\"\"\n",
    "  print(f\"Creating new serving endpoint: {config['serving_endpoint_name']}\")\n",
    "  endpoint_url = f'https://{serving_host}/api/2.0/serving-endpoints'\n",
    "  headers = { 'Authorization': f'Bearer {creds.token}' }\n",
    "  request_data = {\"name\": config['serving_endpoint_name'], \"config\": {\"served_models\": served_models}}\n",
    "  json_bytes = json.dumps(request_data).encode('utf-8')\n",
    "  response = requests.post(endpoint_url, data=json_bytes, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  wait_for_endpoint()\n",
    "  displayHTML(f\"\"\"Created the <a href=\"/#mlflow/endpoints/{config['serving_endpoint_name']}\" target=\"_blank\">{config['serving_endpoint_name']}</a> serving endpoint\"\"\")\n",
    "  \n",
    "def update_endpoint():\n",
    "  \"\"\"Update serving endpoint and wait for it to be ready\"\"\"\n",
    "  print(f\"Updating existing serving endpoint: {config['serving_endpoint_name']}\")\n",
    "  endpoint_url = f\"https://{serving_host}/api/2.0/serving-endpoints/{config['serving_endpoint_name']}/config\"\n",
    "  headers = { 'Authorization': f'Bearer {creds.token}' }\n",
    "  request_data = { \"served_models\": served_models, \"traffic_config\": traffic_config }\n",
    "  json_bytes = json.dumps(request_data).encode('utf-8')\n",
    "  response = requests.put(endpoint_url, data=json_bytes, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  wait_for_endpoint()\n",
    "  displayHTML(f\"\"\"Updated the <a href=\"/#mlflow/endpoints/{config['serving_endpoint_name']}\" target=\"_blank\">{config['serving_endpoint_name']}</a> serving endpoint\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c414c82-61e6-4ea3-b4b7-a7c38d4d592b",
     "showTitle": true,
     "title": "Use the defined function to create or update the endpoint"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new serving endpoint: icbf_llm-qabot-endpoint\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\nWaiting 30s for deployment or update to finish\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created the <a href=\"/#mlflow/endpoints/icbf_llm-qabot-endpoint\" target=\"_blank\">icbf_llm-qabot-endpoint</a> serving endpoint"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created the <a href=\"/#mlflow/endpoints/icbf_llm-qabot-endpoint\" target=\"_blank\">icbf_llm-qabot-endpoint</a> serving endpoint",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gather other inputs the API needs\n",
    "serving_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "creds = get_databricks_host_creds()\n",
    "\n",
    "# kick off endpoint creation/update\n",
    "if not endpoint_exists():\n",
    "  create_endpoint()\n",
    "else:\n",
    "  update_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb06449-481e-47bd-a610-b54affc6d71f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Step 2: Test Endpoint API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03106d97-9e5f-4dad-aadd-de4456159b28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we can use the code below to setup a function to query this endpoint.  This code is a slightly modified version of the code accessible through the *Query Endpoint* UI accessible through the serving endpoint page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7405f752-eb18-497e-aa37-7ce7ed0ff2da",
     "showTitle": true,
     "title": "Define Functions to Query the Endpoint"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "endpoint_url = f\"\"\"https://{serving_host}/serving-endpoints/{config['serving_endpoint_name']}/invocations\"\"\"\n",
    "\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {\n",
    "        \"inputs\": {name: data[name].tolist() for name in data.keys()}\n",
    "        if isinstance(data, dict)\n",
    "        else data.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "def score_model(dataset):\n",
    "    url = endpoint_url\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {creds.token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    ds_dict = (\n",
    "        {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n",
    "        if isinstance(dataset, pd.DataFrame)\n",
    "        else create_tf_serving_json(dataset)\n",
    "    )\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "    response = requests.request(method=\"POST\", headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a865f3-b3c3-47d1-b713-6ac338920b35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And now we can test the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b29654-2d82-4388-ac56-bba29b2e7710",
     "showTitle": true,
     "title": "Test the Model Serving Endpoint"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: {'predictions': [{'answer': 'Una demanda de alimentos es un proceso legal en el que se busca obtener el pago de la cuota de alimentos de una persona obligada a brindarlos. Este proceso se lleva a cabo en el ámbito civil y se adelanta ante el Juez de Familia competente. En la demanda, se puede solicitar el embargo de bienes y derechos del progenitor obligado a brindar alimentos. El Juez tiene la facultad de ordenar al pagador o empleador de esa persona que descuente y consigne a órdenes del juzgado una parte del salario mensual del demandado, así como de sus prestaciones sociales.',\n   'source': 'https://www.icbf.gov.co/cual-es-la-diferencia-entre-el-proceso-ejecutivo-de-alimentos-y-el-de-inasistencia-alimentaria',\n   'output_metadata': {'token_usage': {'prompt_tokens': 387,\n     'completion_tokens': 141,\n     'total_tokens': 528},\n    'model_name': 'gpt-3.5-turbo'}}]}"
     ]
    }
   ],
   "source": [
    "# assemble question input\n",
    "queries = pd.DataFrame({'question':[\n",
    "  \"Que es una demanda de alimentos?\"\n",
    "]})\n",
    "\n",
    "score_model( \n",
    "   queries\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccaf04e1-d359-4512-8469-0bf2463ca301",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some observed limitations:\n",
    "* If we allow the endpoint to scale to zero, we will save cost when the bot is not queried. However, the first request after a long pause can take a few minutes, as it will require the endpoint to scale up from zero nodes\n",
    "* The timeout limit for a serverless model serving request is 60 seconds. If more than 3 questions are submitted in the same request, the model may time out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa3e98e0-be91-42c4-a967-9e63de4724b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "© 2023 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| langchain | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/langchain/ |\n",
    "| tiktoken | Fast BPE tokeniser for use with OpenAI's models | MIT  |   https://pypi.org/project/tiktoken/ |\n",
    "| faiss-cpu | Library for efficient similarity search and clustering of dense vectors | MIT  |   https://pypi.org/project/faiss-cpu/ |\n",
    "| openai | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/openai/ |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Deploy_API",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
