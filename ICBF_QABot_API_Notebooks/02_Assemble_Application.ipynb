{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e79d397-7883-4041-afab-c4439c5817dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to define and persist the model to be used by the QA Bot accelerator.  This notebook is available at https://github.com/databricks-industry-solutions/diy-llm-qa-bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c146a9-ddb4-4080-ae3e-e6401bf7dad9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Introduction\n",
    "\n",
    "With our documents indexed, we can now focus our attention on assembling the core application logic.  This logic will have us retrieve a document from our vector store based on a user-provided question.  That question along with the document, added to provide context, will then be used to assemble a prompt which will then be sent to a model in order to generate a response. </p>\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/bot_application.png' width=900>\n",
    "\n",
    "</p>\n",
    "In this notebook, we'll first walk through these steps one at a time so that we can wrap our head around what all is taking place.  We will then repackage the logic as a class object which will allow us to more easily encapsulate our work.  We will persist that object as a model within MLflow which will assist us in deploying the model in the last notebook associated with this accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ed33b4-0fa5-4f02-bcd2-fd7c9107e6c0",
     "showTitle": true,
     "title": "Install Required Libraries"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting langchain==0.0.250\n  Using cached langchain-0.0.250-py3-none-any.whl (1.4 MB)\nCollecting tiktoken==0.4.0\n  Using cached tiktoken-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\nCollecting openai==0.27.6\n  Using cached openai-0.27.6-py3-none-any.whl (71 kB)\nCollecting faiss-cpu==1.7.4\n  Using cached faiss_cpu-1.7.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\nCollecting typing-inspect==0.8.0\n  Using cached typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\nCollecting typing_extensions==4.5.0\n  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\nCollecting openapi-schema-pydantic<2.0,>=1.2\n  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\nRequirement already satisfied: pydantic<2,>=1 in /databricks/python3/lib/python3.9/site-packages (from langchain==0.0.250) (1.10.2)\nCollecting langsmith<0.1.0,>=0.0.11\n  Using cached langsmith-0.0.21-py3-none-any.whl (32 kB)\nCollecting numexpr<3.0.0,>=2.8.4\n  Using cached numexpr-2.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\nCollecting tenacity<9.0.0,>=8.1.0\n  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\nCollecting SQLAlchemy<3,>=1.4\n  Using cached SQLAlchemy-2.0.19-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\nRequirement already satisfied: numpy<2,>=1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (from langchain==0.0.250) (1.21.6)\nRequirement already satisfied: PyYAML>=5.4.1 in /databricks/python3/lib/python3.9/site-packages (from langchain==0.0.250) (6.0)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.9/site-packages (from langchain==0.0.250) (2.27.1)\nCollecting dataclasses-json<0.6.0,>=0.5.7\n  Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nCollecting aiohttp<4.0.0,>=3.8.3\n  Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\nCollecting async-timeout<5.0.0,>=4.0.0\n  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.9/site-packages (from tiktoken==0.4.0) (2022.3.15)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.9/site-packages (from openai==0.27.6) (4.64.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.9/site-packages (from typing-inspect==0.8.0) (0.4.3)\nCollecting frozenlist>=1.1.1\n  Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.250) (2.0.4)\nCollecting yarl<2.0,>=1.0\n  Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\nCollecting aiosignal>=1.1.2\n  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting multidict<7.0,>=4.5\n  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.250) (21.4.0)\nCollecting marshmallow<4.0.0,>=3.18.0\n  Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\nRequirement already satisfied: packaging>=17.0 in /databricks/python3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.250) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.250) (3.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.250) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.250) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.250) (2021.10.8)\nCollecting greenlet!=0.4.17\n  Using cached greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nInstalling collected packages: typing-extensions, multidict, frozenlist, yarl, typing-inspect, marshmallow, greenlet, async-timeout, aiosignal, tenacity, SQLAlchemy, openapi-schema-pydantic, numexpr, langsmith, dataclasses-json, aiohttp, tiktoken, openai, langchain, faiss-cpu\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4c75e023-c1d1-4491-b390-8cd2bf3e7e76\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Not uninstalling tenacity at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4c75e023-c1d1-4491-b390-8cd2bf3e7e76\n    Can't uninstall 'tenacity'. No files were found to uninstall.\nSuccessfully installed SQLAlchemy-2.0.19 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.5.14 faiss-cpu-1.7.4 frozenlist-1.4.0 greenlet-2.0.2 langchain-0.0.250 langsmith-0.0.21 marshmallow-3.20.1 multidict-6.0.4 numexpr-2.8.5 openai-0.27.6 openapi-schema-pydantic-1.2.4 tenacity-8.2.2 tiktoken-0.4.0 typing-extensions-4.5.0 typing-inspect-0.8.0 yarl-1.9.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.0.250 tiktoken==0.4.0 openai==0.27.6 faiss-cpu==1.7.4 typing-inspect==0.8.0 typing_extensions==4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d094bd-740a-44ae-ba17-ae0a7a2d5b77",
     "showTitle": true,
     "title": "Import Required Libraries"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093f0c33-d779-4734-b313-b96b3b8cc763",
     "showTitle": true,
     "title": "Get Config Settings"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./utils/config_utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d92740f-d8bf-4d67-a8bc-489070880f13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: {'kb_documents_path': 'file:/Workspace/Users/aurelia.ayala@factored.ai/ICBF_QABot/data/icbf_knowledge_base.csv',\n 'vector_store_path': '/dbfs/tmp/icbf_qabot/vector_store',\n 'database_name': 'icbf_qabot',\n 'registered_model_name': 'databricks_icbf_qabot',\n 'model_uri': 'models:/databricks_icbf_qabot/production',\n 'openai_embedding_model': 'text-embedding-ada-002',\n 'openai_chat_model': 'gpt-3.5-turbo',\n 'system_message_template': \"You are a helpful bilingual assistant. You are good at helping to answer a question written in Spanish or English based on the context provided, the context is a document in Spanish. If the context does not provide enough relevant information to determine the answer, just say I don't know. If the context is irrelevant to the question, just say I don't know. If you did not find a good answer from the context, just say I don't know. If the query doesn't form a complete question, just say I don't know. If there is a good answer from the context, try to summarize the context to answer the question. If the question is in Spanish, answer in Spanish. If the question is in English, answer in English.\",\n 'human_message_template': 'Given the context: {context}. Answer the question {question}.',\n 'temperature': 0.15,\n 'eval_dataset_path': 'file:/Workspace/Users/aurelia.ayala@factored.ai/ICBF_QABot/data/icbf_knowledge_base.csv',\n 'openai_key_secret_scope': 'solution-accelerator-cicd',\n 'openai_key_secret_key': 'openai_api',\n 'serving_endpoint_name': 'icbf_llm-qabot-endpoint'}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bfb2101-da31-4f37-b3f9-134686ff5dae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Step 1: Explore Answer Generation\n",
    "\n",
    "To get started, let's explore how we will derive an answer in response to a user provide question.  We'll start by defining that question here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0317ab1-93c9-4395-babf-0902ae621922",
     "showTitle": true,
     "title": "Specify Question"
    }
   },
   "outputs": [],
   "source": [
    "question = \"¿Cómo puedo interponer una demanda de alimentos?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8b0299-24f9-4248-b451-cd64c67f6826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using our vector store, assembled in the prior notebook, we will retrieve document chunks relevant to the question: \n",
    "\n",
    "**NOTE** The OpenAI API key used by the OpenAIEmbeddings object is specified in an environment variable set during the earlier `%run` call to get configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9623aab-e315-4e89-a504-0f50b95f34f5",
     "showTitle": true,
     "title": "Retrieve Relevant Documents"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Luego de presentada la solicitud, las partes implicadas en la fijación de la cuota de alimentos, son citadas a una audiencia de conciliación, con el propósito de que lleguen a un acuerdo sobre el monto de la misma. De no lograr acuerdo alguno, la autoridad que este conociendo del trámite, mediante resolución, fijará provisionalmente las obligaciones de alimentos, custodia y visitas, respecto del niño, niña o adolescente. De llegarse a un acuerdo respecto del monto de la cuota de alimentos, se elaborará acta en la que se exprese dicho acuerdo, el Defensor de Familia deberá entregar una copia a las partes interesadas. En caso de incumplimiento en el pago del monto de la cuota de alimentos, el acta de conciliación servirá de prueba para adelantar una demanda ejecutiva de alimentos. El Defensor de Familia solo dará inicio a la demanda si el interesado manifiesta su intención en que sea presentada y así dar inicio al proceso correspondiente. Ahora bien, establecida la cuota alimentaria provisional, el interesado deberá acudir ante la jurisdicción de familia, con el fin de adelantar el correspondiente proceso de fijación de cuota alimentaria. ' metadata={'url': 'https://www.icbf.gov.co/en-que-consiste-el-tramite-de-fijacion-de-cuota-de-alimentos'} \n\npage_content='Para intentar la fijación de la cuota de alimentos tenga en cuenta que: La persona interesada en la fijación de la cuota de alimentos debe presentarse ante cualquiera de las autoridades antes mencionadas, del lugar donde se encuentre el niño, niña o adolescente para realizar la solicitud formalmente. El niño, niña o adolescente que se beneficiará con los alimentos, debe estar reconocido legalmente, por parte del progenitor al que se le van a reclamar alimentos Deben suministrarse los datos de ubicación del progenitor o progenitora obligada a suministrar los alimentos, tales como: lugar de residencia o trabajo, para que pueda ser citado (a). Los implicados en la fijación de la cuota de alimentos deben asistir en la fecha y hora en la que sean citados para la respectiva diligencia de conciliación. ' metadata={'url': 'https://www.icbf.gov.co/que-requisitos-se-deben-cumplir-para-llevar-cabo-la-fijacion-de-cuota-de-alimentos'} \n\npage_content='Cuando no se da cumplimiento al acuerdo de alimentos pactado ante autoridad competente (defensor o comisario de Familia, acuerdo de conciliación privado, centro de conciliación, sentencia judicial), el \\xa0padre, la madre o la persona que tenga bajo su custodia y/o cuidado personal al \\xa0menor de edad tiene dos opciones: Iniciar el proceso Ejecutivo de Alimentos, el cual tiene como objetivo recaudar el pago de las cuotas o sumas que por concepto de alimentos adeuda el obligado. La cuota debe estar previamente establecida, por cuanto se requiere del acta de \\xa0conciliación, o Sentencia, para iniciar este proceso. Presentar denuncia por el delito de Inasistencia Alimentaria ante la Fiscalía General de la Nación, adjuntando Registro Civil de Nacimiento del niño, niña o adolescente, copia de la cédula del padre, la madre o la persona que tenga bajo su custodia y/o cuidado personal al menor de edad que denuncia, acta en la que se \\xa0pactó o acordó la custodia y los alimentos (si se tiene) aclando que este documento no es un requisito obligatorio para la presentación de la denuncia y los datos donde pueda ser ubicado el obligado. La denuncia podrá ser presentada directamente por el interesado, por lo que no requiere abogado. Vale la pena resaltar que mientras no esté fijada la cuota de alimentos, no se podrá iniciar el Proceso Ejecutivo de Alimentos, sin embargo es viable presentar la denuncia por Inasistencia Alimentaria. ' metadata={'url': 'https://www.icbf.gov.co/que-pasa-si-la-persona-obligada-dar-alimentos-no-cumple-con-la-cuota-pactada-en-acta-de-conciliacion'} \n\npage_content='Puede formularse una denuncia penal ante la Fiscalía por el delito de Inasistencia Alimentaria. Para ello, se debe llevar el registro civil de nacimiento del hijo(a), copia de la cédula del padre o madre que denuncia, acta en la que se fijó la custodia y los alimentos, si se tiene, y los datos donde puede ser ubicado el padre o madre obligado(a) o los últimos que se tengan de éste(a). No se requiere que haya cuota fijada para instaurar la denuncia, basta que se pruebe el parentesco con el registro civil de nacimiento del niño(a) y se haga la manifestación de que el padre o la madre, según sea el caso, no cumple con su obligación alimentaria. En la medida de lo posible, se debe informar qué bienes tiene o dónde trabaja el padre o madre que no cumple, con el fin de probar que la ausencia de pago es injustificada. También es recomendable llevar una lista de los gastos y perjuicios que le ha ocasionado a usted y al niño(a) el incumplimiento del padre o madre. Si está incumpliendo un acuerdo suscrito ante una autoridad competente y conociera el paradero del progenitor que incumple, usted podrá iniciar un proceso Ejecutivo de Alimentos a través del Centro Zonal del ICBF más cercano al lugar donde se encuentre el niño, niña o adolescente, para ello deberá adjuntar registro civil de su hijo(a), copia del acta del acuerdo y dirección del domicilio del que incumple. ' metadata={'url': 'https://www.icbf.gov.co/si-el-padre-o-la-madre-de-mi-hijoa-presenta-un-incumplimiento-reiterado-de-su-obligacion-de-dar'} \n\npage_content='La denuncia por el delito de Inasistencia Alimentaria puede instaurarse cuando el padre o madre, según sea el caso, se ha sustraído sin justificación alguna del cumplimiento de la obligación alimentaria. Lo que se busca con esta denuncia es que se declare responsable de la comisión de un delito al padre o madre incumplido, sin importar si existen o no bienes embargables en cabeza del obligado(a) a dar alimentos, por cuanto es un deber de la Fiscalía investigar hechos que revistan características de delito. Se recomienda que, por escrito, se realice la denuncia. ' metadata={'url': 'https://www.icbf.gov.co/necesito-demandar-por-alimentos-al-padre-o-la-madre-de-mi-hijoa-le-tengo-proceso-en-comisaria-de'} \n\n"
     ]
    }
   ],
   "source": [
    "# open vector store to access embeddings\n",
    "embeddings = OpenAIEmbeddings(model=config['openai_embedding_model'])\n",
    "vector_store = FAISS.load_local(embeddings=embeddings, folder_path=config['vector_store_path'])\n",
    "\n",
    "# configure document retrieval \n",
    "n_documents = 5 # number of documents to retrieve \n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': n_documents}) # configure retrieval mechanism\n",
    "\n",
    "# get relevant documents\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "for doc in docs: \n",
    "  print(doc,'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "765c9481-d227-4f35-953c-fb3dc166c0ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can now turn our attention to the prompt that we will send to the model.  This prompt needs to include placeholders for the *question* the user will submit and the document that we believe will provide the *context* for answering it.\n",
    "\n",
    "Please note that the prompt consists of multiple prompt elements, defined using [prompt templates](https://python.langchain.com/en/latest/modules/prompts/chat_prompt_template.html).  In a nutshell, prompt templates allow us to define the basic structure of a prompt and more easily substitute variable data into them to trigger a response.  The system message prompt shown here provides instruction to the model about how we want it to respond.  The human message template provides the details about the user-initiated request.\n",
    "\n",
    "The prompts along with the details about the model that will respond to the prompt are encapsulated within an [LLMChain object](https://python.langchain.com/en/latest/modules/chains/generic/llm_chain.html).  This object simply defines the basic structure for resolving a query and returning a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a82b08-8d59-4a7d-b57b-4223021e30ed",
     "showTitle": true,
     "title": "Define Chain to Generate Responses"
    }
   },
   "outputs": [],
   "source": [
    "# define system-level instructions\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(config['system_message_template'])\n",
    "\n",
    "# define human-driven instructions\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(config['human_message_template'])\n",
    "\n",
    "# combine instructions into a single prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# define model to respond to prompt\n",
    "llm = ChatOpenAI(model_name=config['openai_chat_model'], temperature=config['temperature'])\n",
    "\n",
    "# combine prompt and model into a unit of work (chain)\n",
    "qa_chain = LLMChain(\n",
    "  llm = llm,\n",
    "  prompt = chat_prompt\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf4aded-c906-429d-a82a-eba18afb8662",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To actually trigger a response, we will loop through each of our docs from highest to lowest relevance and attempt to elicit a response.  Once we get a valid response, we'll stop.\n",
    "\n",
    "Please note, we aren't providing time-out handling or thoroughly validating the response from the model in this next cell.  We will want to make this logic more robust as we assemble our application class but for now we'll keep it simple to ensure the code is easy to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b32d44-e64c-49f2-89d9-da4785b06291",
     "showTitle": true,
     "title": "Generate a Response"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Cómo puedo interponer una demanda de alimentos? \n Answer: Para interponer una demanda de alimentos, primero debes presentar una solicitud ante la autoridad competente. Luego, las partes implicadas en la fijación de la cuota de alimentos serán citadas a una audiencia de conciliación para intentar llegar a un acuerdo sobre el monto de la cuota. Si no se logra un acuerdo, la autoridad competente fijará provisionalmente las obligaciones de alimentos. En caso de incumplimiento en el pago de la cuota de alimentos, el acta de conciliación servirá como prueba para presentar una demanda ejecutiva de alimentos. Sin embargo, es importante mencionar que el Defensor de Familia solo iniciará la demanda si el interesado manifiesta su intención de presentarla. Después de establecer la cuota alimentaria provisional, el interesado deberá acudir a la jurisdicción de familia para iniciar el proceso correspondiente de fijación de la cuota alimentaria.\n"
     ]
    }
   ],
   "source": [
    "# for each provided document\n",
    "for doc in docs:\n",
    "\n",
    "  # get document text\n",
    "  text = doc.page_content\n",
    "\n",
    "  # generate a response\n",
    "  output = qa_chain.generate([{'context': text, 'question': question}])\n",
    " \n",
    "  # get answer from results\n",
    "  generation = output.generations[0][0]\n",
    "  answer = generation.text\n",
    "\n",
    "  # display answer\n",
    "  if answer is not None:\n",
    "    print(f\"Question: {question}\", '\\n', f\"Answer: {answer}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35f3d9b4-4028-47fd-a134-6dd1c46b7df6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Step 2: Assemble Model for Deployment\n",
    "\n",
    "Having explored the basic steps involved in generating a response, let's wrap our logic in a class to make deployment easier.  Our class will be initialized by passing the LLM model definition, a vector store retriever and a prompt to the class.  The *get_answer* method will serve as the primary method for submitting a question and getting a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402cca8e-f80a-4682-8772-98a062fc6b75",
     "showTitle": true,
     "title": "Define QABot Class"
    }
   },
   "outputs": [],
   "source": [
    "class QABot():\n",
    "\n",
    "\n",
    "  def __init__(self, llm, retriever, prompt):\n",
    "    self.llm = llm\n",
    "    self.retriever = retriever\n",
    "    self.prompt = prompt\n",
    "    self.qa_chain = LLMChain(llm = self.llm, prompt=prompt)\n",
    "    self.abbreviations = { # known abbreviations we want to replace\n",
    "      \"ICBF\": \"Instituto Colombiano de Bienestar Familiar\",\n",
    "      } \n",
    "\n",
    "\n",
    "  def _is_good_answer(self, answer):\n",
    "\n",
    "    ''' check if answer is a valid '''\n",
    "\n",
    "    result = True # default response\n",
    "\n",
    "    badanswer_phrases = [ # phrases that indicate model produced non-answer\n",
    "      \"no information\", \"no context\", \"don't know\", \"no clear answer\", \"sorry\", \n",
    "      \"no answer\", \"no mention\", \"reminder\", \"context does not provide\", \"no helpful answer\", \n",
    "      \"given context\", \"no helpful\", \"no relevant\", \"no question\", \"not clear\",\n",
    "      \"don't have enough information\", \" does not have the relevant information\", \"does not seem to be directly related\",\n",
    "      \"no tengo información\", \"no tengo contexto\", \"no sé\", \"no hay una respuesta clara\", \"disculpa\", \"sin respuesta\",\n",
    "      \"no se menciona\", \"el contexto no proporciona\", \"no hay una respuesta útil\", \"sin relevancia\", \"no hay pregunta\",\n",
    "      \"no es claro\", \"no tengo suficiente información\", \"no existe información relevante\", \"no parece estar directamente relacionado\"\n",
    "      ]\n",
    "    \n",
    "    if answer is None: # bad answer if answer is none\n",
    "      results = False\n",
    "    else: # bad answer if contains badanswer phrase\n",
    "      for phrase in badanswer_phrases:\n",
    "        if phrase in answer.lower():\n",
    "          result = False\n",
    "          break\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "  def _get_answer(self, context, question, timeout_sec=60):\n",
    "\n",
    "    '''' get answer from llm with timeout handling '''\n",
    "\n",
    "    # default result\n",
    "    result = None\n",
    "\n",
    "    # define end time\n",
    "    end_time = time.time() + timeout_sec\n",
    "\n",
    "    # try timeout\n",
    "    while time.time() < end_time:\n",
    "\n",
    "      # attempt to get a response\n",
    "      try: \n",
    "        result =  qa_chain.generate([{'context': context, 'question': question}])\n",
    "        break # if successful response, stop looping\n",
    "\n",
    "      # if rate limit error...\n",
    "      except openai.error.RateLimitError as rate_limit_error:\n",
    "        if time.time() < end_time: # if time permits, sleep\n",
    "          time.sleep(2)\n",
    "          continue\n",
    "        else: # otherwise, raiser the exception\n",
    "          raise rate_limit_error\n",
    "\n",
    "      # if other error, raise it\n",
    "      except Exception as e:\n",
    "        print(f'LLM QA Chain encountered unexpected error: {e}')\n",
    "        raise e\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "  def get_answer(self, question):\n",
    "    ''' get answer to provided question '''\n",
    "\n",
    "    # default result\n",
    "    result = {'answer':None, 'source':None, 'output_metadata':None}\n",
    "\n",
    "    # remove common abbreviations from question\n",
    "    for abbreviation, full_text in self.abbreviations.items():\n",
    "      pattern = re.compile(fr'\\b({abbreviation}|{abbreviation.lower()})\\b', re.IGNORECASE)\n",
    "      question = pattern.sub(f\"{abbreviation} ({full_text})\", question)\n",
    "\n",
    "    # get relevant documents\n",
    "    docs = self.retriever.get_relevant_documents(question)\n",
    "\n",
    "    # for each doc ...\n",
    "    for doc in docs:\n",
    "\n",
    "      # get key elements for doc\n",
    "      text = doc.page_content\n",
    "      source = doc.metadata['url']\n",
    "\n",
    "      # get an answer from llm\n",
    "      output = self._get_answer(text, question)\n",
    " \n",
    "      # get output from results\n",
    "      generation = output.generations[0][0]\n",
    "      answer = generation.text\n",
    "      output_metadata = output.llm_output\n",
    "\n",
    "      # assemble results if not no_answer\n",
    "      if self._is_good_answer(answer):\n",
    "        result['answer'] = answer\n",
    "        result['source'] = source\n",
    "        result['output_metadata'] = output_metadata\n",
    "        break # stop looping if good answer\n",
    "      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55179f72-18a2-4b20-82c5-f96311ff2af6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can test our class using the objects instantiated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ff4dc3-b58c-4d7b-aab2-55380ec252cd",
     "showTitle": true,
     "title": "Test the QABot Class"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: {'answer': 'Para interponer una demanda de alimentos, primero debes presentar una solicitud ante la autoridad competente. Luego, las partes implicadas en la fijación de la cuota de alimentos serán citadas a una audiencia de conciliación para intentar llegar a un acuerdo sobre el monto de la cuota. Si no se logra un acuerdo, la autoridad competente fijará provisionalmente las obligaciones de alimentos. En caso de incumplimiento en el pago de la cuota de alimentos, el acta de conciliación servirá como prueba para presentar una demanda ejecutiva de alimentos. Sin embargo, es importante mencionar que el Defensor de Familia solo iniciará la demanda si el interesado manifiesta su intención de presentarla. Después de establecer la cuota alimentaria provisional, el interesado deberá acudir a la jurisdicción de familia para iniciar el proceso correspondiente de fijación de la cuota alimentaria.',\n 'source': 'https://www.icbf.gov.co/en-que-consiste-el-tramite-de-fijacion-de-cuota-de-alimentos',\n 'output_metadata': {'token_usage': {'prompt_tokens': 470,\n   'completion_tokens': 215,\n   'total_tokens': 685},\n  'model_name': 'gpt-3.5-turbo'}}"
     ]
    }
   ],
   "source": [
    "# instantiate bot object\n",
    "qabot = QABot(llm, retriever, chat_prompt)\n",
    "\n",
    "# get response to question\n",
    "qabot.get_answer(question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feefe899-bba1-40ce-a91e-9bbf06cabf3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Step 3: Persist Model to MLflow\n",
    "\n",
    "With our bot class defined and validated, we can now persist it to MLflow.  MLflow is an open source repository for model tracking and logging.  It's deployed by default with the Databricks platform, making it easy for us to record models with it.\n",
    "\n",
    "While MLflow now [supports](https://www.databricks.com/blog/2023/04/18/introducing-mlflow-23-enhanced-native-llm-support-and-new-features.html) both OpenAI and LangChain model flavors, the fact that we've written custom logic for our bot application means that we'll need to make use of the more generic [pyfunc](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models) model flavor.  This model flavor allows us to write a custom wrapper for our model that gives us considerable control over how our model responds when deployed through standard, MLflow-provided deployment mechanisms. \n",
    "\n",
    "To create a custom MLflow model, all we need to do is define a class wrapper of type *mlflow.pyfunc.PythonModel*. The *__init__* method will initialize an instance of our *QABot* class and persist it to an class variable.  And a *predict* method will serve as the standard interface for generating a response.  That method will receive our inputs as a pandas dataframe but we can write the logic with the knowledge that it will only be receiving one user-provided question at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7822f4-2556-452f-b2a0-b03f4e7fd054",
     "showTitle": true,
     "title": "Define MLflow Wrapper for Model"
    }
   },
   "outputs": [],
   "source": [
    "class MLflowQABot(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "  def __init__(self, llm, retriever, chat_prompt):\n",
    "    self.qabot = QABot(llm, retriever, chat_prompt)\n",
    "\n",
    "  def predict(self, context, inputs):\n",
    "    questions = list(inputs['question'])\n",
    "\n",
    "    # return answer\n",
    "    return [self.qabot.get_answer(q) for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636fda9e-1fca-4bda-a370-a7cf758dfb5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can then instantiate our model and log it to the [MLflow registry](https://docs.databricks.com/mlflow/model-registry.html) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1918a02-ca7b-46a5-b720-76ae105bdf59",
     "showTitle": true,
     "title": "Persist Model to MLflow"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nSuccessfully registered model 'databricks_icbf_qabot'.\n2023/08/11 01:21:59 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: databricks_icbf_qabot, version 1\nCreated version '1' of model 'databricks_icbf_qabot'.\n"
     ]
    }
   ],
   "source": [
    "# instantiate mlflow model\n",
    "model = MLflowQABot(llm, retriever, chat_prompt)\n",
    "\n",
    "# persist model to mlflow\n",
    "with mlflow.start_run():\n",
    "  _ = (\n",
    "    mlflow.pyfunc.log_model(\n",
    "      python_model=model,\n",
    "      extra_pip_requirements=['langchain==0.0.250', 'tiktoken==0.4.0', 'openai==0.27.6', 'faiss-cpu==1.7.4', 'typing-inspect==0.8.0', 'typing_extensions==4.5.0'],\n",
    "      artifact_path='model',\n",
    "      registered_model_name=config['registered_model_name']\n",
    "      )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9d528f8-37b3-45df-b6d6-bb32c3bfa59d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you are new to MLflow, you may be wondering what logging is doing for us.  If you navigate to the experiment associated with this notebook - look for the flask icon in the right-hand navigation of your Databricks environment to access the experiments - you can click on the latest experiment to see details about what was recorded with the *log_model* call. If you expand the model artifacts, you should see a *python_model.pkl* file that represents the pickled MLflowQABot model instantiated before.  It's this model that we retrieve when we (later) load our model into this or another environment:\n",
    "</p>\n",
    "\n",
    "<img src=\"https://brysmiwasb.blob.core.windows.net/demos/images/bot_mlflow_log_model.PNG\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85efa2e7-5d86-44d1-b10c-9ddee418bd68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The MLflow model registry provides mechanisms for us to manage our registered models as they move through a CI/CD workflow.  If we want to just push a model straight to production status (which is fine for a demo but not recommended in real-world scenarios), we can do this programmatically as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbd5a5c-ee66-46df-b647-38743567a5f8",
     "showTitle": true,
     "title": "Elevate Model to Production Status"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: <ModelVersion: creation_timestamp=1691716919251, current_stage='Production', description='', last_updated_timestamp=1691716930668, name='databricks_icbf_qabot', run_id='2ff6599ab2cd44b3b8f57a1858a3d29b', run_link='', source='dbfs:/databricks/mlflow-tracking/3872731099569187/2ff6599ab2cd44b3b8f57a1858a3d29b/artifacts/model', status='READY', status_message='', tags={}, user_id='7558745341569247', version='1'>"
     ]
    }
   ],
   "source": [
    "# connect to mlflow \n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# identify latest model version\n",
    "latest_version = client.get_latest_versions(config['registered_model_name'], stages=['None'])[0].version\n",
    "\n",
    "# move model into production\n",
    "client.transition_model_version_stage(\n",
    "    name=config['registered_model_name'],\n",
    "    version=latest_version,\n",
    "    stage='Production',\n",
    "    archive_existing_versions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41d9d22-2757-498e-b69f-7d5ad52cc32d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can then retrieve the model from the registry and submit a few questions to verify the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e9883f-1345-46ae-a5df-0504ddde2c85",
     "showTitle": true,
     "title": "Test the Model"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [{'answer': 'La patria potestad es el conjunto de derechos que la ley les reconoce a ambos padres sobre sus hijos menores de edad, incluyendo la administración de sus bienes y representación legal.',\n  'source': 'https://www.icbf.gov.co/cual-es-la-diferencia-entre-custodia-y-patria-potestad-0',\n  'output_metadata': {'token_usage': {'prompt_tokens': 297,\n    'completion_tokens': 44,\n    'total_tokens': 341},\n   'model_name': 'gpt-3.5-turbo'}},\n {'answer': 'No, según la ley establecida en el contexto, la custodia de los hijos está en primer lugar en cabeza de los padres, incluso si son menores de edad. Sin embargo, si los padres no son aptos para tener la custodia y ofrecer las garantías necesarias para el cuidado del niño, la custodia puede ser otorgada a un familiar.',\n  'source': 'https://www.icbf.gov.co/soy-menor-de-edad-puedo-tener-la-custodia-de-mi-hijo',\n  'output_metadata': {'token_usage': {'prompt_tokens': 263,\n    'completion_tokens': 81,\n    'total_tokens': 344},\n   'model_name': 'gpt-3.5-turbo'}},\n {'answer': 'La prueba de ADN se utiliza para determinar la paternidad o maternidad biológica de un niño, niña o adolescente a través del análisis de su información genética transmitida por los padres.',\n  'source': 'https://www.icbf.gov.co/que-es-la-prueba-de-adn',\n  'output_metadata': {'token_usage': {'prompt_tokens': 288,\n    'completion_tokens': 47,\n    'total_tokens': 335},\n   'model_name': 'gpt-3.5-turbo'}}]"
     ]
    }
   ],
   "source": [
    "# retrieve model from mlflow\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{config['registered_model_name']}/Production\")\n",
    "\n",
    "# assemble question input\n",
    "queries = pd.DataFrame({'question':[\n",
    "  \"¿Qué es la patria potestad?\",\n",
    "  \"¿Puedo tener la custodia de mi hijo si soy menor de edad?\",\n",
    "  \"¿Para qué es una prueba de ADN?\"\n",
    "]})\n",
    "\n",
    "# get a response\n",
    "model.predict(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0df9567-e559-4e95-a489-7b354d558d5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "© 2023 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| langchain | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/langchain/ |\n",
    "| tiktoken | Fast BPE tokeniser for use with OpenAI's models | MIT  |   https://pypi.org/project/tiktoken/ |\n",
    "| faiss-cpu | Library for efficient similarity search and clustering of dense vectors | MIT  |   https://pypi.org/project/faiss-cpu/ |\n",
    "| openai | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/openai/ |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Assemble_Application",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
